data:
  data_dir: "training_dataset" # folder that has *.wav and *.lab data
  sample_rate: 16000 # not doing 44.1kHz cus both the encoders were trained on 16kHz- also its lighter to do ASR with lower sampling rate
  num_val_files: 10
  max_seq_len: null # no limits of phonemes amount in a segment (full segment)
  #frame_duration: 0.02 (this shouldnt be changed)

model:
  encoder_type: "whisper"  # or "wavlm"
  whisper_model: "openai/whisper-base" # look at model_list.txt for more model types
  wavlm_model: "microsoft/wavlm-base" # same here, but the more advanced the base, the bigger and slower it trains
  freeze_encoder: false # false to finetune the encoder, true to not finetune
  
  enable_bilstm: true
  bilstm_num_layer: 2
  
  enable_dilated_conv: true
  dilated_conv_depth: 2
  dilated_conv_kernel: 3

  segmental_loss_weight: 1.0
  segmental_loss_weights: [1.0, 1.0, 2.0] # [start_error, end_error, 1 - IoU]

  subframe_loss_weight: 1.0
  
  num_conformer_layers: 2
  conformer_heads: 2
  conformer_ff_expansion: 2
  conformer_kernel_size: 31
  conformer_dropout: 0.15

  lang_emb_dim: 64
  num_languages: 0 # auto update on save config after preprocess

training:
  batch_size: 1 # 1 for full individual sample per batch, cant set this to anything else (cus not implemented)
  num_workers: 4
  learning_rate: 0.0001
  lr_decay_gamma: 0.98 # learning rate decay gamma every val_check_interval
  weight_decay: 0.00001
  label_smoothing: 0.1 # prevent `overconfident` labels
  max_steps: 500000
  val_check_interval: 2500
  max_checkpoints: 5
  log_dir: "/content/drive/MyDrive/WFL_11/logs" # path to logs folder for tensorboard

finetuning:
  enable: false # enabling WLF finetuning
  model_path: null # path to finetune model

output:
  save_dir: "/content/drive/MyDrive/WFL_11" # path to save folder
  
postprocess:
  median_filter: 2 # 1 for no smoothing, higher the value, the more it smooths the prediction
  merge_segments: "previous"  # right, left, previous, or none
